import json
import pytz
from datetime import datetime
from typing import List, Dict, Any, Tuple

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from openai import OpenAI

from ..config.settings import settings
from ..models import AstroDocument
from ..utils.logger import setup_logger
from .search_service import SearchService
from .service import Service
from .user_settings_service import UserSettingsService

logger = setup_logger(__name__)


class OpenRouterEmbeddings(Embeddings):

    def __init__(self, api_key: str, model: str, base_url: str):
        super().__init__()
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url
        )
        self.model = model

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        response = self.client.embeddings.create(
            model=self.model,
            input=texts,
            encoding_format="float"
        )
        return [item.embedding for item in response.data]

    def embed_query(self, text: str) -> List[float]:
        response = self.client.embeddings.create(
            model=self.model,
            input=[text],
            encoding_format="float"
        )
        return response.data[0].embedding


class AiService(Service):
    def __init__(self, search_service: SearchService, user_settings_service: UserSettingsService):
        super().__init__(logger)
        self.search_service = search_service
        self.user_settings_service = user_settings_service
        self.faiss_index_path = "faiss_index"
        self.vector_store = None
        self.chunk_size = 1000
        self.chunk_overlap = 200
        self.llm_client = OpenAI(
            api_key=settings.openai_api_key,
            base_url="https://openrouter.ai/api/v1"
        )

    async def initialize(self):
        await super().initialize()
        await self.load_faiss_index()

    def _clean_json_response(self, content: str) -> str:
        """
        ĞÑ‡Ğ¸Ñ‰Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚ LLM Ğ¾Ñ‚ markdown Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
        Ğ£Ğ´Ğ°Ğ»ÑĞµÑ‚ ```json Ğ¸ ``` Ğ±Ğ»Ğ¾ĞºĞ¸, ĞµÑĞ»Ğ¸ Ğ¾Ğ½Ğ¸ ĞµÑÑ‚ÑŒ
        """
        content = content.strip()

        # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ markdown Ğ±Ğ»Ğ¾ĞºĞ¸
        if content.startswith("```json"):
            content = content[7:]  # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ```json
        elif content.startswith("```"):
            content = content[3:]  # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ```

        if content.endswith("```"):
            content = content[:-3]  # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ·Ğ°ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ ```

        return content.strip()

    async def update_database(self, force: bool = False):
        try:
            is_outdated = await AstroDocument.is_outdated(days=7)

            if not is_outdated and not force:
                self.logger.info("Astro documents are up to date, skipping update")
                return

            self.logger.info("Astro documents are outdated, updating...")

            await AstroDocument.delete_all()
            self.logger.info("Deleted all old documents")

            query = f"ĞÑÑ‚Ñ€Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ°Ğ»ĞµĞ½Ğ´Ğ°Ñ€ÑŒ Ğ½Ğ° Ğ½ĞµĞ´ĞµĞ»Ñ {datetime.now().strftime('%d.%m.%Y')}"
            doc_count = 0

            for doc_data in self.search_service.search_docs(
                query=query,
                num_results=5,
                fetch_full_content=True
            ):
                content = doc_data['content']
                await AstroDocument.create(content=content)
                doc_count += 1
                self.logger.info(f"Saved document {doc_count}")

            self.logger.info(f"Successfully updated database with {doc_count} documents")

            if doc_count > 0:
                self.logger.info("Creating FAISS vector store...")
                await self._create_faiss_index()
                self.logger.info("FAISS vector store created successfully")

        except Exception as e:
            self.logger.error(f"Failed to update database: {e}")
            raise


    async def load_faiss_index(self):
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° FAISS Ğ¸Ğ½Ğ´ĞµĞºÑĞ° Ğ¸Ğ· Ñ„Ğ°Ğ¹Ğ»Ğ°."""
        import os

        try:
            if not os.path.exists(self.faiss_index_path):
                self.logger.warning(f"FAISS index not found at {self.faiss_index_path}, will create on first update")
                return

            self.logger.info(f"Loading FAISS index from {self.faiss_index_path}")

            embeddings = OpenRouterEmbeddings(
                api_key=settings.openai_api_key,
                model="qwen/qwen3-embedding-8b",
                base_url="https://openrouter.ai/api/v1"
            )

            self.vector_store = FAISS.load_local(
                self.faiss_index_path,
                embeddings,
                allow_dangerous_deserialization=True
            )

            self.logger.info("FAISS index loaded successfully")

        except Exception as e:
            self.logger.error(f"Failed to load FAISS index: {e}")
            self.vector_store = None

    def _split_documents_to_chunks(self, documents: List[Document]) -> List[Document]:
        """Ğ Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ‡Ğ°Ğ½ĞºĞ¸ Ñ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ñ‚Ğ¸ĞµĞ¼."""
        self.logger.info(f"Splitting {len(documents)} documents into chunks (size={self.chunk_size}, overlap={self.chunk_overlap})")

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )

        chunks = text_splitter.split_documents(documents)
        self.logger.info(f"Created {len(chunks)} chunks from {len(documents)} documents")

        return chunks

    async def _create_faiss_index(self):
        try:
            documents = await AstroDocument.all().order_by('-created_at')

            if not documents:
                self.logger.warning("No documents found for FAISS indexing")
                return

            self.logger.info(f"Creating FAISS index from {len(documents)} documents")

            langchain_docs = [
                Document(
                    page_content=doc.content,
                    metadata={
                        "doc_id": doc.id,
                        "created_at": str(doc.created_at)
                    }
                )
                for doc in documents
            ]

            chunks = self._split_documents_to_chunks(langchain_docs)

            for i, chunk in enumerate(chunks):
                chunk.metadata["chunk_id"] = i
                chunk.metadata["chunk_index"] = i

            self.logger.info(f"Creating embeddings for {len(chunks)} chunks...")

            embeddings = OpenRouterEmbeddings(
                api_key=settings.openai_api_key,
                model="qwen/qwen3-embedding-8b",
                base_url="https://openrouter.ai/api/v1"
            )

            self.vector_store = FAISS.from_documents(chunks, embeddings)
            self.vector_store.save_local(self.faiss_index_path)

            self.logger.info(f"FAISS index with {len(chunks)} chunks saved to {self.faiss_index_path}")

        except Exception as e:
            self.logger.error(f"Failed to create FAISS index: {e}")
            raise

    def search_similar_chunks(self, query: str, k: int = 5) -> List[Document]:
        if not self.vector_store:
            self.logger.warning("FAISS index not loaded, cannot search")
            return []

        try:
            results = self.vector_store.similarity_search(query, k=k)

            self.logger.info(f"Found {len(results)} similar chunks for query")
            for i, doc in enumerate(results, 1):
                self.logger.debug(
                    f"Result {i}: doc_id={doc.metadata.get('doc_id')}, "
                    f"chunk_id={doc.metadata.get('chunk_id')}"
                )

            return results

        except Exception as e:
            self.logger.error(f"Failed to search in FAISS index: {e}")
            return []

    async def _get_owner_settings_with_timezone(self) -> Tuple["UserSettings", str, pytz.BaseTzInfo]:
        owner_settings = await self.user_settings_service.get_owner_settings()
        timezone_name = owner_settings.timezone or settings.timezone
        timezone = pytz.timezone(timezone_name)
        return owner_settings, timezone_name, timezone

    async def _ai_parse_datetime_and_name(
        self,
        text: str,
        timezone_name: str,
        timezone: pytz.BaseTzInfo
    ) -> Dict[str, Any]:
        """Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ LLM Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° Ğ´Ğ°Ñ‚Ñ‹, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ."""

        system_prompt = """Ğ¢Ñ‹ - Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° Ğ´Ğ°Ñ‚ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ¢Ğ²Ğ¾Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° - Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¸ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ.

Ğ¢ĞµĞºÑƒÑ‰Ğ¸Ğ¹ Ñ‡Ğ°ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ¾ÑÑ: {timezone}
Ğ¢ĞµĞºÑƒÑ‰ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ: {current_time}

Ğ Ğ°ÑĞ¿Ğ°Ñ€ÑĞ¸ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ²ĞµÑ€Ğ½Ğ¸ JSON Ğ¾Ğ±ÑŠĞµĞºÑ‚ ÑĞ¾ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹:
{{
    "start_datetime": "YYYY-MM-DD HH:MM:SS",  // Ğ”Ğ°Ñ‚Ğ° Ğ¸ Ğ²Ñ€ĞµĞ¼Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ² ISO Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ
    "end_datetime": "YYYY-MM-DD HH:MM:SS",    // ĞĞ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾, Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞµÑĞ»Ğ¸ ÑƒĞºĞ°Ğ·Ğ°Ğ½ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½
    "event_name": "ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ",          // ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ/Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ
    "description": "ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ"                  // ĞĞ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ
}}

ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»Ğ°:
1. Ğ’ÑĞµĞ³Ğ´Ğ° Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°Ğ¹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ğ¹ JSON
2. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ 24-Ñ‡Ğ°ÑĞ¾Ğ²Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ´Ğ»Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸
3. Ğ•ÑĞ»Ğ¸ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ğ½Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ñ‚ĞµĞºÑƒÑ‰ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ
4. Ğ•ÑĞ»Ğ¸ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ°Ñ Ğ´Ğ°Ñ‚Ğ° Ğ½Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ
5. Ğ”Ğ»Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° "Ñ‡ĞµÑ€ĞµĞ· 2 Ñ‡Ğ°ÑĞ°", Ñ€Ğ°ÑÑÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ğ¹ Ğ¾Ñ‚ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸
6. Ğ”Ğ»Ñ Ğ´Ğ½ĞµĞ¹ Ğ½ĞµĞ´ĞµĞ»Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° "Ğ¿ÑÑ‚Ğ½Ğ¸Ñ†Ğ°", Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞµ Ğ²Ñ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ
7. Ğ”Ğ»Ñ "ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ½ĞµĞ´ĞµĞ»ÑŒĞ½Ğ¸Ğº", Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ¿Ğ¾Ğ½ĞµĞ´ĞµĞ»ÑŒĞ½Ğ¸Ğº ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ Ğ½ĞµĞ´ĞµĞ»Ğ¸
8. Ğ•ÑĞ»Ğ¸ ÑƒĞºĞ°Ğ·Ğ°Ğ½ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, "Ñ 14 Ğ´Ğ¾ 16"), ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ğ¹ Ğ¾Ğ±Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸
9. Ğ”ĞµÑ€Ğ¶Ğ¸ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğ¼Ğ¸ Ğ½Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸
10. Ğ•ÑĞ»Ğ¸ Ñ‡ĞµÑ‚ĞºĞ¾Ğµ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ "Ğ‘ĞµĞ· Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ"

ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹:
- "Ğ·Ğ°Ğ²Ñ‚Ñ€Ğ° Ğ² 15:00 Ğ’ÑÑ‚Ñ€ĞµÑ‡Ğ° Ñ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ¾Ğ¹" â†’ {{"start_datetime": "2024-01-16 15:00:00", "event_name": "Ğ’ÑÑ‚Ñ€ĞµÑ‡Ğ° Ñ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ¾Ğ¹"}}
- "Ğ¿ÑÑ‚Ğ½Ğ¸Ñ†Ğ° Ñ 14:00 Ğ´Ğ¾ 16:00 ĞŸÑ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ñƒ" â†’ {{"start_datetime": "2024-01-19 14:00:00", "end_datetime": "2024-01-19 16:00:00", "event_name": "ĞŸÑ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ñƒ"}}
- "Ñ‡ĞµÑ€ĞµĞ· 2 Ñ‡Ğ°ÑĞ° ĞĞ±Ğ·Ğ¾Ñ€ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°" â†’ {{"start_datetime": "2024-01-15 17:30:00", "event_name": "ĞĞ±Ğ·Ğ¾Ñ€ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°"}}
- "ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ½ĞµĞ´ĞµĞ»ÑŒĞ½Ğ¸Ğº Ğ² 10:00 Ğ’ÑÑ‚Ñ€ĞµÑ‡Ğ°" â†’ {{"start_datetime": "2024-01-22 10:00:00", "event_name": "Ğ’ÑÑ‚Ñ€ĞµÑ‡Ğ°"}}
- "tomorrow 3pm Team Meeting" â†’ {{"start_datetime": "2024-01-16 15:00:00", "event_name": "Team Meeting"}}

Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°Ğ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ JSON Ğ¾Ğ±ÑŠĞµĞºÑ‚, Ğ½Ğ¸Ñ‡ĞµĞ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ."""

        current_time = datetime.now(timezone)
        formatted_system_prompt = system_prompt.format(
            timezone=timezone_name,
            current_time=current_time.strftime("%Y-%m-%d %H:%M:%S %Z")
        )

        try:
            response = self.llm_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": formatted_system_prompt},
                    {"role": "user", "content": f"Parse this text: {text}"}
                ],
                temperature=0.1,
                max_tokens=10000
            )

            content = response.choices[0].message.content
            self.logger.info(f"AI response: {content}")

            # ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¾Ñ‚ markdown Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
            cleaned_content = self._clean_json_response(content)
            parsed_data = json.loads(cleaned_content)

            if 'start_datetime' not in parsed_data:
                raise ValueError("AI response missing start_datetime")

            return parsed_data

        except json.JSONDecodeError as e:
            self.logger.error(f"Failed to parse AI response as JSON: {e}")
            raise ValueError("Invalid AI response format")
        except Exception as e:
            self.logger.error(f"LLM API error: {e}")
            raise ValueError(f"AI parsing failed: {e}")

    def _validate_and_convert_datetime(self, datetime_str: str, timezone: pytz.BaseTzInfo) -> datetime:
        """Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ datetime Ğ² timezone-aware Ğ¾Ğ±ÑŠĞµĞºÑ‚."""
        if not datetime_str or datetime_str.strip() == "":
            self.logger.warning("Empty datetime string, using current time")
            now = datetime.now(timezone)
            return now.astimezone(pytz.UTC)

        try:
            dt = datetime.fromisoformat(datetime_str.replace('Z', '+00:00'))

            if dt.tzinfo is None:
                dt = timezone.localize(dt)

            return dt.astimezone(pytz.UTC)

        except Exception as e:
            self.logger.error(f"Failed to validate datetime '{datetime_str}': {e}")
            raise ValueError(f"Invalid datetime format: {datetime_str}")

    def _parse_reminder_times(self, reminder_str: str) -> List[int]:
        """ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· ÑÑ‚Ñ€Ğ¾ĞºĞ¸."""
        from ..utils.helpers import parse_reminder_time

        reminder_times = []
        self.logger.info(f"Reminder string: {reminder_str}")
        if reminder_str is None or len(reminder_str) == 0:
            return [15, 60]

        for time_str in reminder_str.split(','):
            time_str = time_str.strip()
            if time_str:
                try:
                    minutes = parse_reminder_time(time_str)
                    reminder_times.append(minutes)
                except Exception:
                    self.logger.warning(f"Could not parse reminder time: {time_str}")

        return reminder_times

    async def process_event_with_astro_context(self, event_data: dict) -> dict:
        self.logger.info(f"Processing event with astro context: {event_data.get('event_name')}")

        try:
            event_datetime = event_data['event_datetime']
            event_name = event_data['event_name']

            event_date_str = event_datetime.strftime('%d %B %Y')
            search_query = f"ĞÑÑ‚Ñ€Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ· Ğ½Ğ° {event_date_str}. Ğ¡Ğ¾Ğ±Ñ‹Ñ‚Ğ¸Ğµ: {event_name}"
            self.logger.info(f"Searching astro context with query: {search_query}")

            relevant_chunks = self.search_similar_chunks(search_query, k=5)

            if not relevant_chunks:
                self.logger.warning("No astro context found, returning fallback message")
                event_data['result'] = "OK"
                event_data['message'] = "ĞÑÑ‚Ñ€Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ· Ğ½Ğ° ÑÑ‚Ğ¾Ñ‚ Ğ´ĞµĞ½ÑŒ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½"
                return event_data

            context_parts = []
            for i, chunk in enumerate(relevant_chunks, 1):
                context_parts.append(f"[Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº {i}]:\n{chunk.page_content}")
            context = "\n\n".join(context_parts)

            astro_prompt = self._create_astro_analysis_prompt(event_data, context)

            self.logger.info("Requesting astro analysis from LLM...")
            response = self.llm_client.chat.completions.create(
                model="openai/gpt-4o-mini",
                messages=[{"role": "user", "content": astro_prompt}],
                temperature=0.3,
                max_tokens=2000,
            )

            astro_response = response.choices[0].message.content
            self.logger.info(f"Received astro analysis raw: {astro_response}")

            cleaned_response = self._clean_json_response(astro_response)
            self.logger.info(f"Cleaned astro analysis: {cleaned_response}")
            astro_analysis = json.loads(cleaned_response)

            event_data['result'] = astro_analysis.get('result', 'OK')
            event_data['message'] = "ğŸ”® ĞÑÑ‚Ñ€Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ¾Ğ²ĞµÑ‚:\n" + astro_analysis.get('message', "")

            self.logger.info(f"Final event data with astro: {event_data}")
            return event_data

        except Exception as e:
            self.logger.error(f"Failed to process astro context: {e}")
            event_data['result'] = "OK"
            event_data['message'] = "ĞÑÑ‚Ñ€Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ· Ğ½Ğ° ÑÑ‚Ğ¾Ñ‚ Ğ´ĞµĞ½ÑŒ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½"
            return event_data


    async def _ai_classify_is_event(self, text: str) -> dict:
        """
        ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚: ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ»Ğ¸ Ğ¾Ğ½ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ¼ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ.
        Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ JSON: { "is_event": true/false, "reason": "..." }
        """
        prompt = f"""
    Ğ¢Ñ‹ â€” Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ»Ğ¸ Ñ‚ĞµĞºÑÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ / Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ¸ / Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.

    Ğ’ĞµÑ€Ğ½Ğ¸ JSON ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ:
    {{
        "is_event": true/false,
        "reason": "ĞºÑ€Ğ°Ñ‚ĞºĞ¾Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğµ"
    }}

    Ğ¢ĞµĞºÑÑ‚: "{text}"

    ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»Ğ°:
    - is_event = true, ĞµÑĞ»Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ñ…Ğ¾Ñ‡ĞµÑ‚ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ÑŒ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ñƒ, ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğµ, Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ, Ğ½Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ.
    - is_event = false, ĞµÑĞ»Ğ¸ Ñ‚ĞµĞºÑÑ‚ â€” Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€, Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ, Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ, Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ‚.Ğ¿.
    - Ğ•ÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ ÑĞ»Ğ°Ğ±Ñ‹Ğ¹ Ğ½Ğ°Ğ¼Ñ‘Ğº Ğ½Ğ° â€œĞ½Ğ°Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ÑŒâ€, â€œĞ²ÑÑ‚Ñ€ĞµÑ‚Ğ¸Ñ‚ÑŒÑÑâ€, â€œĞ¿Ğ¾Ğ·Ğ²Ğ¾Ğ½Ğ¸Ñ‚ÑŒâ€, â€œĞ·Ğ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒÑÑâ€, â€œÑ‡ĞµÑ€ĞµĞ· 2 Ñ‡Ğ°ÑĞ° ...â€, â€” ÑÑ‚Ğ°Ğ²ÑŒ true.
    - Ğ’ÑĞµĞ³Ğ´Ğ° Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°Ğ¹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ğ¹ JSON.
    """

        try:
            response = self.llm_client.chat.completions.create(
                model="openai/gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                max_tokens=200
            )

            content = response.choices[0].message.content
            cleaned = self._clean_json_response(content)
            data = json.loads(cleaned)
            is_event = bool(data.get("is_event", False))
            self.logger.info(f"Event classification for '{text}': {is_event}")
            return is_event
        except Exception as e:
            self.logger.error(f"Failed to classify text as event/non-event: {e}")
            # Ğ’ ÑĞ»ÑƒÑ‡Ğ°Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ½Ğ¸Ñ‡ĞµĞ³Ğ¾ Ğ½Ğµ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ, Ñ‡ĞµĞ¼ Ğ»Ğ¾Ğ¼Ğ°Ñ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ
            return False

    def _create_astro_analysis_prompt(self, event_data: dict, astro_context: str) -> str:
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ğ´Ğ»Ñ Ğ°ÑÑ‚Ñ€Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ."""

        event_datetime = event_data['event_datetime']
        event_name = event_data['event_name']
        event_description = event_data.get('description', '')
        timezone_name = event_data.get('timezone', settings.timezone)
        timezone = pytz.timezone(timezone_name)

        # Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ°Ñ‚Ñƒ Ğ¸ Ğ²Ñ€ĞµĞ¼Ñ
        local_datetime = event_datetime.astimezone(timezone)
        date_str = local_datetime.strftime('%d %B %Y')
        time_str = local_datetime.strftime('%H:%M')
        weekday_str = local_datetime.strftime('%A')

        prompt = f"""Ğ¢Ñ‹ â€” Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°ÑÑ‚Ñ€Ğ¾Ğ»Ğ¾Ğ³. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑÑ‚Ñ€Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ°Ğ¹ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğ¹ ÑĞ¾Ğ²ĞµÑ‚ Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¸.

Ğ˜ĞĞ¤ĞĞ ĞœĞĞ¦Ğ˜Ğ¯ Ğ Ğ¡ĞĞ‘Ğ«Ğ¢Ğ˜Ğ˜:
ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ: {event_name}
Ğ”Ğ°Ñ‚Ğ°: {date_str} ({weekday_str})
Ğ’Ñ€ĞµĞ¼Ñ: {time_str}
ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ: {event_description if event_description else 'ĞĞµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¾'}

ĞĞ¡Ğ¢Ğ ĞĞ›ĞĞ“Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ™ ĞšĞĞĞ¢Ğ•ĞšĞ¡Ğ¢:
{astro_context}

Ğ—ĞĞ”ĞĞ§Ğ:
ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ¿Ñ€Ğ¸ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑÑ‚Ñ€Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°.

Ğ¢Ğ²Ğ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ:
1. ĞšÑ€Ğ°Ñ‚ĞºĞ¸Ğ¼ (2-4 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ)
2. ĞšĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ (Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ÑŒÑÑ Ğ¸Ğ¼ĞµĞ½Ğ½Ğ¾ Ğº ÑÑ‚Ğ¾Ğ¼Ñƒ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸)
3. ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ (Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸)
4. ĞÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ°ÑÑ‚Ñ€Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ
5. Ğ”Ñ€ÑƒĞ¶ĞµĞ»ÑĞ±Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğ¼ (Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ°ÑÑ‚Ñ€Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ², Ğ¿Ğ¸ÑˆĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ¼)
6. ĞŸĞ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼ (Ğ´Ğ°Ğ¶Ğµ ĞµÑĞ»Ğ¸ Ğ²Ñ€ĞµĞ¼Ñ Ğ½Ğµ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾)
7. Ğ•ÑĞ»Ğ¸ Ğ²Ñ€ĞµĞ¼Ñ Ğ½Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¸Ñ‚, Ñ‚Ğ¾ Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ñƒ

ĞĞµ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¸, ÑÑ€Ğ°Ğ·Ñƒ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¸ Ğº Ğ°ÑÑ‚Ñ€Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ.

Ğ’ÑĞµĞ³Ğ´Ğ° ÑÑ‚Ñ€ĞµĞ¼Ğ¸ÑÑŒ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ¸ ÑÑ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼Ñ ÑĞºĞ¾Ñ€ĞµĞµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¼, ĞµÑĞ»Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ½Ğµ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° ÑĞ²Ğ½Ñ‹Ğµ Ñ€Ğ¸ÑĞºĞ¸. 


Ğ’ĞµÑ€Ğ½Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ JSON ÑĞ¾ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹:
{{
    "result": "OK/BAD",
    "message": "ĞÑÑ‚Ñ€Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸"
}}
result Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ OK Ğ¸Ğ»Ğ¸ BAD, ĞµÑĞ»Ğ¸ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¸Ñ‚, Ñ‚Ğ¾ OK, ĞµÑĞ»Ğ¸ Ğ½ĞµÑ‚, Ñ‚Ğ¾ BAD
message Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿ÑƒÑÑ‚Ñ‹Ğ¼

ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹:
- {{"result": "OK", "message": "ĞÑÑ‚Ñ€Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ¾Ğ²ĞµÑ‚: ÑÑ‚Ğ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ"}}
- {{"result": "BAD", "message": "Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ğ³Ğ¾Ñ€Ğ¾ÑĞºĞ¾Ğ¿Ñƒ, Ğ½ĞµĞ´ĞµĞ»Ñ Ñ 3 Ğ¿Ğ¾ 9 Ğ½Ğ¾ÑĞ±Ñ€Ñ 2025 Ğ³Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ·Ğ½Ğ°ĞºĞ° Ğ’Ğ¾Ğ´Ğ¾Ğ»ĞµĞ¹ Ğ½Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ°,
    Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ·Ğ½Ğ°ĞºĞ° Ğ¡ĞºĞ¾Ñ€Ğ¿Ğ¸Ğ¾Ğ½ ÑÑ‚Ğ° Ğ½ĞµĞ´ĞµĞ»Ñ â€” Ğ²Ñ€ĞµĞ¼Ñ Ğ¼ÑƒĞ´Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ·Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¾ ÑĞµĞ±Ğµ, Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ ÑĞ»ÑƒÑˆĞ°Ñ‚ÑŒ ÑĞ²Ğ¾Ñ‘ ÑĞµÑ€Ğ´Ñ†Ğµ Ğ¸ Ğ½Ğµ ÑƒÑĞ»Ğ¾Ğ¶Ğ½ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.
    Ğ­Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¾ Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑĞµĞ¹Ñ‡Ğ°Ñ Ğ½Ğµ ÑĞ°Ğ¼Ğ¾Ğµ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ¿Ñ€Ğ¸ÑÑ‚Ğ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ğ´Ğ»Ñ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ²ÑÑ‚Ñ€ĞµÑ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹.
    ĞĞ°Ğ¿ÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ: Ğ¿Ğ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ Ğ¿ĞµÑ€ĞµĞ½ĞµÑÑ‚Ğ¸ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ñƒ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ¿Ñ€Ğ¸ÑÑ‚Ğ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ½Ğ° ÑĞ»ĞµĞ´ÑƒÑÑ‰ÑƒÑ Ğ½ĞµĞ´ĞµĞ»Ñ."}}
- {{"result": "OK", "message": "ĞÑÑ‚Ñ€Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ¾Ğ²ĞµÑ‚: Ğ—Ğ°Ğ²Ñ‚Ñ€Ğ°ÑˆĞ½Ğ¸Ğµ Ñ‚Ñ€Ğ°Ğ½Ğ·Ğ¸Ñ‚Ñ‹ Ğ²Ñ‹Ğ³Ğ»ÑĞ´ÑÑ‚ ÑĞ¿Ğ¾ĞºĞ¾Ğ¹Ğ½Ñ‹Ğ¼Ğ¸ â€” Ğ´Ğ°Ğ¶Ğµ ĞµÑĞ»Ğ¸ Ğ´ĞµĞ½ÑŒ Ğ² Ñ†ĞµĞ»Ğ¾Ğ¼ ĞºĞ°Ğ¶ĞµÑ‚ÑÑ ÑĞ½ĞµÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½ĞµÑ€Ğ¾Ğ²Ğ½Ñ‹Ğ¼, Ğ² Ğ²Ğ°ÑˆĞµĞ¹ Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¶Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ³Ğ»Ğ¸ Ğ±Ñ‹ Ğ¿Ğ¾Ğ¼ĞµÑˆĞ°Ñ‚ÑŒ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğµ. Ğ’Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚ ÑĞºĞ¾Ñ€ĞµĞµ Ğ½ĞµĞ¹Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ, Ñ‚Ğ°Ğº Ñ‡Ñ‚Ğ¾ ÑĞ¼ĞµĞ»Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ¹Ñ‚Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğµ: Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ĞµÑ‰Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¹Ñ‚Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ¿Ñ€Ğ¸ÑÑ‚Ğ½Ñ‹Ñ… ÑÑÑ€Ğ¿Ñ€Ğ¸Ğ·Ğ¾Ğ²."}}

"""

        return prompt

    async def parse_event_command(self, command_text: str) -> dict:
        await self.update_database(force=False)

        try:
            self.logger.info(f"Parsing command text with AI: '{command_text}'")

            text = command_text.strip()
            if text.startswith('++event'):
                text = text[7:].strip()

            self.logger.info(f"Text after removing ++event prefix: '{text}'")

            is_event = await self._ai_classify_is_event(text)
            if not is_event:
                self.logger.info("Text is not classified as event, doing nothing")
                return None

            # TODO somewhere here you can fetch owner's birthday and use it to calculate the best time for the event
            owner_settings, timezone_name, timezone = await self._get_owner_settings_with_timezone()

            reminder_times = []
            if '--remind' in text:
                parts = text.split('--remind')
                text = parts[0].strip()
                self.logger.info(f'After split: text="{text}", reminder_str="{parts[1].strip() if len(parts) > 1 else ""}"')
                if len(parts) > 1:
                    reminder_str = parts[1].strip()
                    reminder_times = self._parse_reminder_times(reminder_str)

            if not reminder_times:
                reminder_times = list(owner_settings.default_reminder_times)

            parsed_data = await self._ai_parse_datetime_and_name(text, timezone_name, timezone)

            event_datetime = self._validate_and_convert_datetime(parsed_data.get('start_datetime'), timezone)
            end_datetime = None
            if parsed_data.get('end_datetime'):
                end_datetime = self._validate_and_convert_datetime(parsed_data.get('end_datetime'), timezone)

            event_name = parsed_data.get('event_name', 'Untitled Event')

            event_data = {
                'event_name': event_name,
                'description': parsed_data.get('description', ""),
                'event_datetime': event_datetime,
                'end_datetime': end_datetime,
                'reminder_times': reminder_times,
                'timezone': timezone_name
            }

            logger.info(f"Event data: {event_data}")

            return await self.process_event_with_astro_context(event_data)

        except Exception as e:
            self.logger.error(f"Failed to parse event command with AI: {e}")
            raise
